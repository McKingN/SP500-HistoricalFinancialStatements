{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone --branch master https://github.com/McKingN/SP500-HistoricalFinancialStatements.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:09:15.929585Z","iopub.execute_input":"2024-12-01T17:09:15.930009Z","iopub.status.idle":"2024-12-01T17:09:18.274201Z","shell.execute_reply.started":"2024-12-01T17:09:15.929970Z","shell.execute_reply":"2024-12-01T17:09:18.272973Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'SP500-HistoricalFinancialStatements'...\nremote: Enumerating objects: 103, done.\u001b[K\nremote: Counting objects: 100% (103/103), done.\u001b[K\nremote: Compressing objects: 100% (30/30), done.\u001b[K\nremote: Total 103 (delta 68), reused 100 (delta 67), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (103/103), 342.58 KiB | 3.17 MiB/s, done.\nResolving deltas: 100% (68/68), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd SP500-HistoricalFinancialStatements","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:09:20.404695Z","iopub.execute_input":"2024-12-01T17:09:20.405093Z","iopub.status.idle":"2024-12-01T17:09:20.413087Z","shell.execute_reply.started":"2024-12-01T17:09:20.405057Z","shell.execute_reply":"2024-12-01T17:09:20.411828Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/SP500-HistoricalFinancialStatements\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!cd data/SP500_components_statements;ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:09:21.797089Z","iopub.execute_input":"2024-12-01T17:09:21.797489Z","iopub.status.idle":"2024-12-01T17:09:22.883486Z","shell.execute_reply.started":"2024-12-01T17:09:21.797454Z","shell.execute_reply":"2024-12-01T17:09:22.881889Z"}},"outputs":[{"name":"stdout","text":"ABBV.json  ACN.json   AES.json\tAMD.json  MMM.json\nABT.json   ADBE.json  AFL.json\tAOS.json\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:09:24.576395Z","iopub.execute_input":"2024-12-01T17:09:24.576826Z","iopub.status.idle":"2024-12-01T17:09:36.375795Z","shell.execute_reply.started":"2024-12-01T17:09:24.576788Z","shell.execute_reply":"2024-12-01T17:09:36.374450Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.2.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 2)) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->-r requirements.txt (line 2)) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\nfrom collections import defaultdict\nimport os\nimport json\nimport pandas as pd\nimport requests\n\n\ndef verify_and_find_next_json(directory):\n    \"\"\"\n    Vérifie les fichiers JSON dans un dossier et détermine le prochain nom de fichier JSON.\n\n    :param directory: Chemin vers le dossier contenant les fichiers JSON.\n    :return: Une chaîne correspondant au prochain fichier JSON (\"i.json\" ou \"j.json\").\n    \"\"\"\n    # Liste des fichiers dans le dossier\n    files = os.listdir(directory)\n    \n    # Filtrer les fichiers qui ont un nom correspondant à \"N.json\" avec N un entier\n    json_files = [f for f in files if f.endswith('.json') and f[:-5].isdigit()]\n    \n    if not json_files:\n        # Si le dossier est vide ou aucun fichier JSON valide trouvé\n        return \"1.json\"\n    \n    # Extraire les entiers des noms de fichiers (par exemple, \"1.json\" -> 1)\n    json_indices = sorted(int(f[:-5]) for f in json_files)\n    \n    # Vérifier s'il manque un numéro dans la séquence de 1 à max\n    for i in range(1, max(json_indices) + 1):\n        if i not in json_indices:\n            return f\"{i}.json\"\n    \n    # Si tous les fichiers de 1 à max existent, retourner \"max + 1\"\n    if max(json_indices) == 54 :\n        print(\"Le dossier est complet. Merci pour votre aide\")\n        return\n    return f\"{max(json_indices) + 1}.json\"\n\ndef split_json(input_file, output_folder, chunk_size=8):\n    \"\"\"\n    Divise un fichier JSON en plusieurs fichiers avec un nombre limité d'éléments.\n\n    :param input_file: Chemin vers le fichier JSON source.\n    :param output_folder: Dossier où enregistrer les fichiers divisés.\n    :param chunk_size: Nombre maximal d'éléments par fichier.\n    \"\"\"\n    # Vérifie si le dossier de sortie existe, sinon le crée\n    os.makedirs(output_folder, exist_ok=True)\n    \n    # Charge les données du fichier JSON\n    with open(input_file, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    # Obtenir toutes les clés et les diviser en groupes\n    keys = list(data.keys())\n    chunks = [keys[i:i + chunk_size] for i in range(0, len(keys), chunk_size)]\n    \n    # Crée un fichier pour chaque groupe\n    for i, chunk in enumerate(chunks, start=1):\n        chunk_data = {key: data[key] for key in chunk}\n        output_file = os.path.join(output_folder, f\"{i}.json\")\n        \n        # Enregistre le groupe dans un fichier JSON\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(chunk_data, f, indent=4)\n        \n        print(f\"Fichier créé : {output_file}\")\n\ndef fetch_financial_data(ticker, api_key, output_folder):\n    \"\"\"\n    Récupère les données financières (état des résultats, bilan, et flux de trésorerie)\n    pour un ticker donné via l'API Alpha Vantage. Les données sont sauvegardées dans\n    un fichier JSON nommé \"ticker.json\" avant d'être retournées sous forme de dictionnaire.\n\n    :param ticker: Le symbole boursier du titre.\n    :param api_key: Clé API pour accéder à l'API Alpha Vantage.\n    :param output_folder: Chemin vers le dossier où sauvegarder les données JSON.\n    :return: Un dictionnaire contenant les données financières.\n    \"\"\"\n    base_url = \"https://www.alphavantage.co/query\"\n    functions = [\"INCOME_STATEMENT\", \"BALANCE_SHEET\", \"CASH_FLOW\"]\n    financial_data = {}\n\n    # Vérifie si le dossier de sortie existe, sinon le crée\n    os.makedirs(output_folder, exist_ok=True)\n\n    for func in functions:\n        params = {\n            \"function\": func,\n            \"symbol\": ticker,\n            \"apikey\": api_key\n        }\n        response = requests.get(base_url, params=params)\n        if response.status_code == 200:\n            financial_data[func.lower()] = response.json()\n        else:\n            print(f\"Échec de la récupération de {func} pour {ticker}: {response.status_code}\")\n            financial_data[func.lower()] = None\n\n    # Chemin du fichier de sortie\n    output_file = os.path.join(output_folder, f\"{ticker}.json\")\n    \n    # Sauvegarde des données financières dans un fichier JSON\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(financial_data, f, indent=4)\n    \n    print(f\"Données sauvegardées dans {output_file}\")\n\n    return financial_data\n\n\ndef process_financial_data(financial_data, ticker):\n    \"\"\"\n    Process raw financial data to merge income statement, balance sheet, and cash flow.\n    Returns a dictionary of financial data aggregated by fiscal year.\n    \"\"\"\n    # Extract reports from the raw financial data\n    income_reports = financial_data.get(\"income_statement\", {}).get(\"annualReports\", [])\n    balance_reports = financial_data.get(\"balance_sheet\", {}).get(\"annualReports\", [])\n    cashflow_reports = financial_data.get(\"cash_flow\", {}).get(\"annualReports\", [])\n\n    # Convert lists to DataFrames\n    income_df = pd.DataFrame(income_reports)\n    balance_df = pd.DataFrame(balance_reports)\n    cashflow_df = pd.DataFrame(cashflow_reports)\n\n    # Merge data on fiscalDateEnding\n    combined_df = income_df.merge(balance_df, on=\"fiscalDateEnding\", suffixes=('_income', '_balance'))\n    combined_df = combined_df.merge(cashflow_df, on=\"fiscalDateEnding\", suffixes=('', '_cashflow'))\n\n    # Convert numeric fields to floats\n    for col in combined_df.columns:\n        if col != \"fiscalDateEnding\":\n            combined_df[col] = pd.to_numeric(combined_df[col], errors=\"coerce\")\n\n    # Create a dictionary with fiscalDateEnding as keys\n    combined_data = combined_df.set_index(\"fiscalDateEnding\").to_dict(orient=\"index\")\n\n    # Add the ticker to each entry\n    for date in combined_data:\n        combined_data[date][\"ticker\"] = ticker\n\n    return combined_data\n\n\ndef aggregate_financial_data(input_json_path, api_key, output_json_path, outDir):\n    \"\"\"\n    Aggregate financial data for tickers listed in the input JSON file.\n    Outputs a JSON file with combined financial data for all tickers by fiscal year.\n    \"\"\"\n    # Load tickers from input JSON file\n    with open(input_json_path, \"r\") as file:\n        tickers_data = json.load(file)\n\n    aggregated_data = defaultdict(dict)\n\n    for ticker, details in tickers_data.items():\n        print(f\"Processing {ticker}...\")\n        financial_data = fetch_financial_data(ticker, api_key, outDir)\n        if all(financial_data.values()):  # Check if all API calls were successful\n            processed_data = process_financial_data(financial_data, ticker)\n            for date, data in processed_data.items():\n                aggregated_data[date][ticker] = data\n        else:\n            print(f\"Skipping {ticker} due to missing data.\")\n\n    # Save aggregated data to JSON\n    with open(output_json_path, \"w\") as output_file:\n        json.dump(aggregated_data, output_file, indent=4)\n\n    print(f\"Aggregated financial data saved to {output_json_path}\")\n\ndef main(api, components_dir=\"data/FilteredSP500_components_splitted\", results_dir=\"data/SP500_components_CombinedStatements\", saveStockStatementsDir=\"data/SP500_components_statements\"):\n    inputFileName = verify_and_find_next_json(results_dir)\n    inputFilePath = os.path.join(components_dir, inputFileName)\n    outputFilePath = os.path.join(results_dir, inputFileName)\n    aggregate_financial_data(input_json_path=inputFilePath, api_key=api, output_json_path=outputFilePath, outDir=saveStockStatementsDir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:10:58.011131Z","iopub.execute_input":"2024-12-01T17:10:58.011589Z","iopub.status.idle":"2024-12-01T17:10:58.037166Z","shell.execute_reply.started":"2024-12-01T17:10:58.011550Z","shell.execute_reply":"2024-12-01T17:10:58.036103Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"if __name__ == '__main__' :\n    API_KEY=\"I8WTUZSKCJ2GM9Q4\"\n    main(API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:40:18.898340Z","iopub.execute_input":"2024-12-01T17:40:18.898716Z","iopub.status.idle":"2024-12-01T17:40:22.721666Z","shell.execute_reply.started":"2024-12-01T17:40:18.898682Z","shell.execute_reply":"2024-12-01T17:40:22.720206Z"}},"outputs":[{"name":"stdout","text":"Processing AZO...\nDonnées sauvegardées dans data/SP500_components_statements/AZO.json\nProcessing AVB...\nDonnées sauvegardées dans data/SP500_components_statements/AVB.json\nProcessing AVY...\nDonnées sauvegardées dans data/SP500_components_statements/AVY.json\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m :\n\u001b[1;32m      2\u001b[0m     API_KEY\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI8WTUZSKCJ2GM9Q4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 177\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(api, components_dir, results_dir, saveStockStatementsDir)\u001b[0m\n\u001b[1;32m    175\u001b[0m inputFilePath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(components_dir, inputFileName)\n\u001b[1;32m    176\u001b[0m outputFilePath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_dir, inputFileName)\n\u001b[0;32m--> 177\u001b[0m \u001b[43maggregate_financial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_json_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputFilePath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_json_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputFilePath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutDir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaveStockStatementsDir\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 161\u001b[0m, in \u001b[0;36maggregate_financial_data\u001b[0;34m(input_json_path, api_key, output_json_path, outDir)\u001b[0m\n\u001b[1;32m    159\u001b[0m financial_data \u001b[38;5;241m=\u001b[39m fetch_financial_data(ticker, api_key, outDir)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(financial_data\u001b[38;5;241m.\u001b[39mvalues()):  \u001b[38;5;66;03m# Check if all API calls were successful\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_financial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinancial_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m date, data \u001b[38;5;129;01min\u001b[39;00m processed_data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    163\u001b[0m         aggregated_data[date][ticker] \u001b[38;5;241m=\u001b[39m data\n","Cell \u001b[0;32mIn[7], line 137\u001b[0m, in \u001b[0;36mprocess_financial_data\u001b[0;34m(financial_data, ticker)\u001b[0m\n\u001b[1;32m    134\u001b[0m         combined_df[col] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(combined_df[col], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Create a dictionary with fiscalDateEnding as keys\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfiscalDateEnding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Add the ticker to each entry\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m combined_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:2178\u001b[0m, in \u001b[0;36mDataFrame.to_dict\u001b[0;34m(self, orient, into, index)\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;124;03mConvert the DataFrame to a dictionary.\u001b[39;00m\n\u001b[1;32m   2077\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2174\u001b[0m \u001b[38;5;124;03m defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\u001b[39;00m\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mto_dict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_dict\n\u001b[0;32m-> 2178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/methods/to_dict.py:242\u001b[0m, in \u001b[0;36mto_dict\u001b[0;34m(df, orient, into, index)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame index must be unique for orient=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m     columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m are_all_object_dtype_cols:\n","\u001b[0;31mValueError\u001b[0m: DataFrame index must be unique for orient='index'."],"ename":"ValueError","evalue":"DataFrame index must be unique for orient='index'.","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"!cd data/SP500_components_statements;ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:11:15.829116Z","iopub.execute_input":"2024-12-01T17:11:15.829531Z","iopub.status.idle":"2024-12-01T17:11:16.912195Z","shell.execute_reply.started":"2024-12-01T17:11:15.829497Z","shell.execute_reply":"2024-12-01T17:11:16.910844Z"}},"outputs":[{"name":"stdout","text":"A.json\t   ACN.json   AFL.json\t ALGN.json  AOS.json  MMM.json\nABBV.json  ADBE.json  AKAM.json  ALLE.json  APD.json\nABT.json   AES.json   ALB.json\t AMD.json   ARE.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!cd data/SP500_components_statements;ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T15:16:10.441792Z","iopub.execute_input":"2024-12-01T15:16:10.442197Z","iopub.status.idle":"2024-12-01T15:16:11.615507Z","shell.execute_reply.started":"2024-12-01T15:16:10.442166Z","shell.execute_reply":"2024-12-01T15:16:11.614016Z"}},"outputs":[{"name":"stdout","text":"A.json\t   ABT.json  ADBE.json\tAFL.json  AOS.json  MMM.json\nABBV.json  ACN.json  AES.json\tAMD.json  APD.json\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Path to the directory containing the files\nfolder_path = '/kaggle/working/SP500-HistoricalFinancialStatements/data/SP500_components_statements/'\n\n# Create a ZIP file to store all files\nzip_filename = '/kaggle/working/output_files.zip'\n\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    # Walk through the folder and add all files to the ZIP file\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_path))\n\n# Provide the download link for the ZIP file\nFileLink(zip_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:39:25.853786Z","iopub.execute_input":"2024-12-01T17:39:25.854233Z","iopub.status.idle":"2024-12-01T17:39:25.936263Z","shell.execute_reply.started":"2024-12-01T17:39:25.854191Z","shell.execute_reply":"2024-12-01T17:39:25.935226Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output_files.zip","text/html":"<a href='/kaggle/working/output_files.zip' target='_blank'>/kaggle/working/output_files.zip</a><br>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Path to the directory containing the files\nfolder_path = '/kaggle/working/SP500-HistoricalFinancialStatements/data/SP500_components_CombinedStatements/'\n\n# Create a ZIP file to store all files\nzip_filename = '/kaggle/working/output_files.zip'\n\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    # Walk through the folder and add all files to the ZIP file\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_path))\n\n# Provide the download link for the ZIP file\nFileLink(zip_filename)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:41:26.576414Z","iopub.execute_input":"2024-12-01T17:41:26.576840Z","iopub.status.idle":"2024-12-01T17:41:26.599141Z","shell.execute_reply.started":"2024-12-01T17:41:26.576804Z","shell.execute_reply":"2024-12-01T17:41:26.598034Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/output_files.zip","text/html":"<a href='/kaggle/working/output_files.zip' target='_blank'>/kaggle/working/output_files.zip</a><br>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}